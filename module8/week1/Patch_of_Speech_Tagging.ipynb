{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "fhO9LbkWW0H3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nFUHl8xUaqB",
        "outputId": "db8cfa16-2498-4596-e18a-f42f96311690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import torch\n",
        "import evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "\n",
        "# load tree bank dataset\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        "print('Number of samples: ', len(tagged_sentences))\n",
        "\n",
        "# save sentences and tags\n",
        "sentences, sentence_tags = [], []\n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append([word.lower() for word in sentence])\n",
        "    sentence_tags.append([tag for tag in tags])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmTYgBHKW4Ez",
        "outputId": "ed267918-d46f-4ecc-8f38-cff10297fee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples:  3914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaHm4CzpAnQV",
        "outputId": "1ebcfe55-3965-4a9d-f68d-0245932f83b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pierre',\n",
              " 'vinken',\n",
              " ',',\n",
              " '61',\n",
              " 'years',\n",
              " 'old',\n",
              " ',',\n",
              " 'will',\n",
              " 'join',\n",
              " 'the',\n",
              " 'board',\n",
              " 'as',\n",
              " 'a',\n",
              " 'nonexecutive',\n",
              " 'director',\n",
              " 'nov.',\n",
              " '29',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "WhH19hkzXk66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, test_sentences, train_tags, test_tags = train_test_split(\n",
        "    sentences,\n",
        "    sentence_tags,\n",
        "    test_size=0.3\n",
        ")\n",
        "\n",
        "valid_sentences, test_sentences, valid_tags, test_tags = train_test_split(\n",
        "    test_sentences,\n",
        "    test_tags,\n",
        "    test_size=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "-8mXkx0JW3-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization and modeling\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "MAX_LEN = 256\n",
        "model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=True,\n",
        ")\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4YHUPsbOW37l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6400710-bc22-404d-e6da-b508d5af98e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PosTagging_Dataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 sentences: List[List[str]],\n",
        "                 tags: List[List[str]],\n",
        "                 tokenizer,\n",
        "                 label2id,\n",
        "                 max_len=MAX_LEN):\n",
        "        super().__init__()\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_token = self.sentences[idx]\n",
        "        label_token = self.tags[idx]\n",
        "\n",
        "        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n",
        "        attention_mask = [1] * len(input_token)\n",
        "        labels = [self.label2id[token] for token in label_token]\n",
        "\n",
        "        return {\n",
        "            'input_ids': self.pad_and_truncate(input_token, pad_id=self.tokenizer.pad_token_id),\n",
        "            'labels': self.pad_and_truncate(labels, pad_id=self.label2id['O']),\n",
        "            'attention_mask': self.pad_and_truncate(attention_mask, pad_id=0)\n",
        "        }\n",
        "\n",
        "    def pad_and_truncate(self, inputs: List[int], pad_id: int):\n",
        "        if len(inputs) < self.max_len:\n",
        "            padded_inputs = inputs + [pad_id] * (self.max_len - len(inputs))\n",
        "        else:\n",
        "            padded_inputs = inputs[:self.max_len]\n",
        "        return torch.as_tensor(padded_inputs)"
      ],
      "metadata": {
        "id": "DKS_rPElW34x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label2id, id2label\n",
        "from collections import defaultdict\n",
        "\n",
        "label2id = defaultdict(int, model.config.label2id)\n",
        "id2label = {id: tag for tag, id in label2id.items()}\n",
        "label2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcRTsTMEBMgj",
        "outputId": "d1fc2e66-bda9-40d8-d71b-82467828ecd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'#': 7,\n",
              "             '$': 6,\n",
              "             \"''\": 5,\n",
              "             ',': 2,\n",
              "             '-LRB-': 17,\n",
              "             '-RRB-': 32,\n",
              "             '.': 4,\n",
              "             ':': 3,\n",
              "             'CC': 8,\n",
              "             'CD': 9,\n",
              "             'DT': 10,\n",
              "             'EX': 11,\n",
              "             'FW': 12,\n",
              "             'IN': 13,\n",
              "             'JJ': 14,\n",
              "             'JJR': 15,\n",
              "             'JJS': 16,\n",
              "             'LS': 18,\n",
              "             'MD': 19,\n",
              "             'NN': 20,\n",
              "             'NNP': 21,\n",
              "             'NNPS': 22,\n",
              "             'NNS': 23,\n",
              "             'O': 0,\n",
              "             'PDT': 24,\n",
              "             'POS': 25,\n",
              "             'PRP': 26,\n",
              "             'PRP$': 27,\n",
              "             'RB': 28,\n",
              "             'RBR': 29,\n",
              "             'RBS': 30,\n",
              "             'RP': 31,\n",
              "             'SYM': 33,\n",
              "             'TO': 34,\n",
              "             'UH': 35,\n",
              "             'VB': 36,\n",
              "             'VBD': 37,\n",
              "             'VBG': 38,\n",
              "             'VBN': 39,\n",
              "             'VBP': 40,\n",
              "             'VBZ': 41,\n",
              "             'WDT': 42,\n",
              "             'WP': 43,\n",
              "             'WP$': 44,\n",
              "             'WRB': 45,\n",
              "             '``': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PosTagging_Dataset(train_sentences, train_tags, tokenizer, label2id)\n",
        "val_dataset = PosTagging_Dataset(valid_sentences, valid_tags, tokenizer, label2id)\n",
        "test_dataset = PosTagging_Dataset(test_sentences, test_tags, tokenizer, label2id)"
      ],
      "metadata": {
        "id": "CemkmCCUW312"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric"
      ],
      "metadata": {
        "id": "0DUGF_Jmaene"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "ignore_label = len(label2id)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    mask = labels != ignore_label\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    return accuracy.compute(\n",
        "        predictions=predictions[mask].tolist(),\n",
        "        references=labels[mask].tolist()\n",
        "    )"
      ],
      "metadata": {
        "id": "6-XmuBUzW3tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "bNqJ0amra4ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import wandb\n",
        "wandb.init(mode='disabled')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='out_dir',\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "o7P8Nh_YW3qF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "5170e146-2783-44f9-a9be-ce1131b0cf3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e8aac0fd02e1>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1720/1720 24:16, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.048083</td>\n",
              "      <td>0.986485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.039083</td>\n",
              "      <td>0.988661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.148700</td>\n",
              "      <td>0.034849</td>\n",
              "      <td>0.989892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.148700</td>\n",
              "      <td>0.033177</td>\n",
              "      <td>0.990431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.148700</td>\n",
              "      <td>0.032277</td>\n",
              "      <td>0.990710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.031400</td>\n",
              "      <td>0.031618</td>\n",
              "      <td>0.990850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.031400</td>\n",
              "      <td>0.030979</td>\n",
              "      <td>0.991016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.031400</td>\n",
              "      <td>0.030590</td>\n",
              "      <td>0.991169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.030565</td>\n",
              "      <td>0.991216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.030579</td>\n",
              "      <td>0.991263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1720, training_loss=0.0625857719155245, metrics={'train_runtime': 1457.4068, 'train_samples_per_second': 18.794, 'train_steps_per_second': 1.18, 'total_flos': 3579882599208960.0, 'train_loss': 0.0625857719155245, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "d1Dhcj_ybdfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = 'We are exploring the topic of deep learning'\n",
        "input_sentence = torch.as_tensor([tokenizer.convert_tokens_to_ids(test_sentence.split())])\n",
        "input_sentence = input_sentence.to('cuda')\n",
        "\n",
        "# predictions\n",
        "outputs = model(input_sentence)\n",
        "_, preds = torch.max(outputs.logits, -1)\n",
        "preds = preds[0].cpu().numpy()\n",
        "\n",
        "# decode\n",
        "pred_tags = ''\n",
        "for pred in preds:\n",
        "    pred_tags += id2label[pred] + ' '\n",
        "pred_tags"
      ],
      "metadata": {
        "id": "BHZlvGMfW3nZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4fc1cb3f-1113-4fad-d1b5-063b83ec576b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PRP VBP RB DT NN IN JJ NN '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}